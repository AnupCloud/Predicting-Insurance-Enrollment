# Insurance Enrollment Prediction API

This project provides an API for predicting employee insurance enrollment probabilities using a logistic regression model. The API is built with FastAPI and can process both individual employee data and batch predictions via JSON requests or CSV file uploads.

## Table of Contents

- [Installation](#installation)
- [uv setup](#1-install-uv)
- [Usage + Docker](#usage)
- [API Endpoints](#api-endpoints)
- [Model Information](#model-information)
- [Project Structure](#project-structure)
- [Example Requests](#example-requests)
- [Data Observations](#data-observations)
- [Model Choice & Rationale](#model-choice--rationale)
- [Evaluation Results](#evaluation-results)
- [Key Takeaways](#key-takeaways)
- [Future Work](#future-work)
- [Technical Implementation](#technical-implementation)

## Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/yourusername/insurance-enrollment-api.git
    cd insurance-enrollment-api
    ```
    *(Replace `yourusername` with the actual username/organization)*

**üìå Setting Up the Project Using UV**

--------------------------------------

UV is a fast Python package manager that helps manage dependencies efficiently.

### **1Ô∏è‚É£ Install UV**

` brew install uv `

### **2Ô∏è‚É£ Create a New UV Project**

` uv init insurance ` # Creates a new project named "hello-world" 

` cd insurance ` # To go inside the directory

### **3Ô∏è‚É£ Project Structure**

After initialization, your project structure will look like this:
```
‚îú‚îÄ‚îÄ .python-version 
‚îú‚îÄ‚îÄ README.md 
‚îú‚îÄ‚îÄ main.py 
‚îî‚îÄ‚îÄ pyproject.toml 
```

### **4Ô∏è‚É£ Running the Project**

Run the project using uv:

` uv run main.py `

**üì¶ Managing Dependencies in UV**

----------------------------------

UV allows you to manage dependencies in your **pyproject.toml** file.

### **5Ô∏è‚É£ Installing Dependencies**

To add a package (e.g., requests):

` uv add requests `

### **6Ô∏è‚É£ Removing Dependencies**

To remove a package (e.g., requests):

` uv remove requests `

### **7Ô∏è‚É£ Install Dependencies from requirements.txt**

If you have a requirements.txt file, install all dependencies at once:

` uv add -r requirements.txt `

**üõ† Virtual Environment & Project Structure**

----------------------------------------------

Once you start adding dependencies, UV will create a **virtual environment (.venv)**, and your project structure will update:

```
‚îú‚îÄ‚îÄ .venv 
‚îÇ   ‚îú‚îÄ‚îÄ bin 
‚îÇ   ‚îú‚îÄ‚îÄ lib 
‚îÇ   ‚îî‚îÄ‚îÄ pyvenv.cfg 
‚îú‚îÄ‚îÄ .python-version 
‚îú‚îÄ‚îÄ README.md 
‚îú‚îÄ‚îÄ main.py 
‚îú‚îÄ‚îÄ pyproject.toml 
‚îî‚îÄ‚îÄ uv.lock 
```

### **8Ô∏è‚É£ Example pyproject.toml Configuration**

This file defines the project metadata and dependencies.

``` 
[project] 
name = "hello-world" 
version = "0.1.0" 
description = "Add your description here" 
readme = "README.md" 
dependencies = [] 
```

2**Ensure the trained model file is in the correct location:**
    Make sure `final_model.pkl` exists within the `output_logistic/` directory:
    ```
    output_logistic/final_model.pkl
    ```

## Usage
### Please run : ``` python src/pipeline_process.py``` 
### To get the model saved inside folder name ```src/output_logistic```


1.  **Start the API server:**
    Using the main script:
    ```bash
    python main.py
    ```
    Or using uvicorn directly (useful for development with auto-reload):
    ```bash
    uvicorn main:app --reload
    ```

2.  **Access the API documentation:**
    The API provides interactive documentation generated by FastAPI:
    -   **Swagger UI:** [http://localhost:8000/docs](http://localhost:8000/docs)
    -   **ReDoc:** [http://localhost:8000/redoc](http://localhost:8000/redoc)

3.  **Use the API endpoints** to make predictions as shown in the [Example Requests](#example-requests) section.



## **üê≥ Running the Project Inside a Docker Container**

----------------------------------------------------

If you want to **containerize the project** using **Docker**, follow these steps:

### **9Ô∏è‚É£ Create a Dockerfile**

Run:

` touch Dockerfile `

Then, add the following content:
```
FROM python:3.12-slim
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

COPY . /app

WORKDIR /app
RUN uv add -r requirements.txt
RUN uv sync --frozen --no-cache 

CMD ["/app/.venv/bin/uvicorn", "main:app", "--port", "8000", "--host","0.0.0.0", "--port", "8000"]
```

**üî® Build & Run the Docker Container**

---------------------------------------

### **üîπ 10Ô∏è‚É£ Build the Docker Image**

` docker build -t insurance . `

Here, **fastapi-app** is the name of your Docker image.

### **üîπ 11Ô∏è‚É£ Run the Container**

` docker run -p 8000:8000 insurance `

Your FastAPI app will now be accessible at:üëâ **http://localhost:8000**

To view API documentation, open:

* **Swagger UI:** http://localhost:8000/docs

* **Redoc UI:** http://localhost:8000/redoc

## API Endpoints

-   `GET /`: Root endpoint with basic information about the API.
-   `GET /health`: Health check endpoint to verify if the API is running.
-   `GET /model-info`: Get information about the trained model (e.g., features used, type).
-   `POST /predict`: Predict enrollment probability for a single employee via JSON payload.
-   `POST /predict-batch`: Predict enrollment probability for multiple employees via JSON payload.
-   `POST /predict-from-csv`: Upload a CSV file containing employee data for batch predictions.

## Model Information

The prediction model is a **logistic regression** classifier trained on employee data to predict the likelihood of insurance enrollment.

**Features Used:**

-   Age
-   Gender
-   Marital status
-   Salary
-   Employment type (Full-time, Part-time, Contract)
-   Region
-   Dependency status (Has dependents: Yes/No)
-   Tenure years

The model was trained with `L2 regularization` and `class balancing` techniques to handle potential imbalanced data effectively.

## Project Structure
```
‚îú‚îÄ‚îÄ main.py # FastAPI application logic
‚îú‚îÄ‚îÄ requirements.txt # Project dependencies
‚îú‚îÄ‚îÄ src # all the code related modules (data, ouput_logistics, config.py, pipeline_process.py)
‚îî‚îÄ‚îÄ final_model.pkl # Trained and serialized logistic regression model file
‚îú‚îÄ‚îÄ README.md # Project documentation (this file)
‚îú‚îÄ‚îÄ output_logistic/ # Directory containing model artifacts
‚îÇ ‚îî‚îÄ‚îÄ final_model.pkl # Trained and serialized logistic regression model file
‚îî‚îÄ‚îÄ # (Other potential files like preprocessing scripts, notebooks, data folder etc.)
```
## Example Requests

### Single Employee Prediction

```bash
curl -X 'POST' \
  'http://localhost:8000/predict' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "employee_id": 20001,
  "age": 35,
  "gender": "Male",
  "marital_status": "Married",
  "salary": 75000,
  "employment_type": "Full-time",
  "region": "West",
  "has_dependents": "Yes",
  "tenure_years": 5.5
}'
```

### Batch Prediction
```bash
curl -X 'POST' \
  'http://localhost:8000/predict-batch' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "employees": [
    {
      "employee_id": 20001,
      "age": 35,
      "gender": "Male",
      "marital_status": "Married",
      "salary": 75000,
      "employment_type": "Full-time",
      "region": "West",
      "has_dependents": "Yes",
      "tenure_years": 5.5
    },
    {
      "employee_id": 20002,
      "age": 50,
      "gender": "Female",
      "marital_status": "Single",
      "salary": 85000,
      "employment_type": "Part-time",
      "region": "Northeast",
      "has_dependents": "No",
      "tenure_years": 2.0
    }
  ]
}'
```

### Predict from CSV
#### Use the /docs endpoint interface to test CSV upload or construct a curl command like:
```bash
curl -X 'POST' \
  'http://localhost:8000/predict-from-csv' \
  -H 'accept: application/json' \
  -F 'file=@/path/to/your/employee_data.csv'
```
(Note: The CSV should have columns matching the required features)


Data Observations
Dataset Composition: Detailed analysis of the dataset's composition, including feature types (numeric, categorical) and target variable (enrollment status) distribution.

Key Patterns: Insights into key patterns discovered in the data, such as potential correlations between features like employment_type, salary, age and insurance enrollment.

Data Quality: Notes on data quality aspects like missing values, outliers, and the preprocessing steps required (e.g., imputation, encoding).

Model Choice & Rationale
Model Selection: Logistic Regression was chosen as the primary model. Its advantages include:

High interpretability (easy to understand feature coefficients).

Computational efficiency for training and prediction.

Good baseline performance for binary classification tasks.

Outputs probabilities, which is suitable for this use case.

Preprocessing Approach:

Categorical Features: One-hot encoding applied to features like gender, marital_status, employment_type, region, has_dependents.

Numeric Features: Standard scaling (mean 0, variance 1) applied to features like age, salary, tenure_years.

Class Imbalance: Addressed using techniques like setting class_weight='balanced' in the Logistic Regression model or using over/under-sampling methods if needed.

Hyperparameter Tuning: GridSearchCV or a similar technique was employed to find the optimal regularization strength (C parameter) for the logistic regression model, balancing bias and variance.

Evaluation Results
Metrics: The model's performance was assessed using standard classification metrics:

Accuracy: Overall correct predictions.

ROC AUC: Ability to distinguish between classes.

Precision: Accuracy of positive predictions.

Recall (Sensitivity): Ability to identify actual positive cases.

F1 Score: Harmonic mean of Precision and Recall.

Feature Importance: Analysis of the model's coefficients revealed the most influential features driving enrollment predictions (both positively and negatively). For example, higher salary might positively correlate, while certain employment types might negatively correlate.

Validation Strategy: A robust validation strategy, likely k-fold cross-validation, was used during training and tuning to ensure the model generalizes well to unseen data and avoid overfitting.

Key Takeaways
Influential Features: Features such as salary, age, employment_type, and has_dependents often show significant influence on insurance enrollment decisions.

Preprocessing is Critical: Proper handling of categorical features (encoding), scaling of numeric features, and addressing class imbalance are essential steps for building a reliable logistic regression model.

Model Performance: Logistic Regression, when appropriately preprocessed and tuned, provides a solid and interpretable baseline for predicting insurance enrollment.

Future Work
Data Enhancements:

Incorporate additional relevant features (e.g., department, job level, previous insurance status).

Perform temporal analysis if historical enrollment data is available.

Integrate external data sources (e.g., regional health indices, economic indicators).

Model Improvements:

Explore more complex models like ensemble methods (Random Forests, Gradient Boosting machines - XGBoost, LightGBM) for potentially higher accuracy.

Implement more sophisticated feature engineering techniques.

Experiment with different hyperparameter tuning strategies.

Deployment Enhancements:

Implement model monitoring for performance degradation and data drift detection.

Add model explainability tools (e.g., SHAP, LIME) to provide insights into individual predictions.

Develop a more user-friendly interface (optional).

Containerize the application (e.g., using Docker) for easier deployment.

Technical Implementation
API Framework: The API is built using FastAPI, a modern, high-performance Python web framework ideal for building APIs. It offers automatic data validation, interactive documentation, and asynchronous capabilities.

End-to-End Workflow: The project encompasses the typical machine learning workflow: data loading and exploration, preprocessing, model training (logistic regression), evaluation, model serialization, and deployment via the FastAPI application.

Dependencies: All necessary Python libraries are listed in the requirements.txt file. Key libraries include fastapi, uvicorn, scikit-learn, pandas, and potentially others for specific preprocessing or data handling tasks.
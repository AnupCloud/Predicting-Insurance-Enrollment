{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# to download the output folder as zip\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files as colab_files  # Rename to avoid conflict\n",
        "\n",
        "# Path to visualization output directory\n",
        "viz_dir = '/content/output_comparative'\n",
        "\n",
        "# Check if directory exists\n",
        "if not os.path.exists(viz_dir):\n",
        "    print(f\"Directory {viz_dir} does not exist\")\n",
        "else:\n",
        "    # Create a zip file of the visualization outputs\n",
        "    zip_filename = 'output_comparative.zip'\n",
        "\n",
        "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "        # Add all files from the directory to the zip file\n",
        "        for root, dirs, files in os.walk(viz_dir):\n",
        "            for file in files:\n",
        "                # Get the full file path\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Add file to zip (with path relative to viz_dir)\n",
        "                arcname = os.path.relpath(file_path, start=os.path.dirname(viz_dir))\n",
        "                zipf.write(file_path, arcname=arcname)\n",
        "\n",
        "    print(f\"Created zip file with {len(zipf.namelist())} files\")\n",
        "\n",
        "    # Download the zip file\n",
        "    colab_files.download(zip_filename)\n",
        "    print(f\"Downloaded {zip_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "w9s7UrWR1Hr2",
        "outputId": "a756afaa-aba9-463c-eb1f-ded04eaecbdc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created zip file with 9 files\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_79ba3cfa-7dc7-4bec-8d7a-baac90b9c0ad\", \"output_comparative.zip\", 1029417)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded output_comparative.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, log_loss\n",
        "import joblib\n",
        "import os\n",
        "import time\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs('output_logistic', exist_ok=True)\n",
        "\n",
        "def create_learning_curves(model, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Generate learning curves for the logistic regression model\n",
        "\n",
        "    Args:\n",
        "        model: Trained model object\n",
        "        X_train, y_train: Training data\n",
        "        X_val, y_val: Validation data\n",
        "    \"\"\"\n",
        "    # Set up figures for accuracy and loss\n",
        "    fig_acc, ax_acc = plt.subplots(figsize=(10, 6))\n",
        "    fig_loss, ax_loss = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Training sizes to evaluate (from 10% to 100% of training data)\n",
        "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
        "\n",
        "    # Calculate learning curves for accuracy\n",
        "    train_sizes_abs, train_scores, val_scores = learning_curve(\n",
        "        model, X_train, y_train,\n",
        "        train_sizes=train_sizes,\n",
        "        cv=5, scoring='accuracy',\n",
        "        n_jobs=-1)\n",
        "\n",
        "    # Calculate mean and std for training and validation scores\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    val_mean = np.mean(val_scores, axis=1)\n",
        "    val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "    # Plot accuracy learning curve\n",
        "    ax_acc.plot(train_sizes_abs, train_mean, 'o-', color='r', label='Training accuracy')\n",
        "    ax_acc.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')\n",
        "    ax_acc.plot(train_sizes_abs, val_mean, 'o-', color='g', label='Validation accuracy')\n",
        "    ax_acc.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.1, color='g')\n",
        "    ax_acc.set_title('Learning Curve - Accuracy')\n",
        "    ax_acc.set_xlabel('Training Examples')\n",
        "    ax_acc.set_ylabel('Accuracy')\n",
        "    ax_acc.legend(loc='best')\n",
        "    ax_acc.grid(True)\n",
        "\n",
        "    # Calculate log loss for different training sizes\n",
        "    train_loss = np.zeros(len(train_sizes_abs))\n",
        "    val_loss = np.zeros(len(train_sizes_abs))\n",
        "\n",
        "    for j, train_size in enumerate(train_sizes_abs):\n",
        "        # Subsample the training data\n",
        "        X_train_subset, _, y_train_subset, _ = train_test_split(\n",
        "            X_train, y_train, train_size=train_size/len(X_train), random_state=42)\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "        # Calculate log loss on training and validation sets\n",
        "        train_prob = model.predict_proba(X_train_subset)\n",
        "        val_prob = model.predict_proba(X_val)\n",
        "\n",
        "        train_loss[j] = log_loss(y_train_subset, train_prob)\n",
        "        val_loss[j] = log_loss(y_val, val_prob)\n",
        "\n",
        "    # Plot loss learning curve\n",
        "    ax_loss.plot(train_sizes_abs, train_loss, 'o-', color='r', label='Training loss')\n",
        "    ax_loss.plot(train_sizes_abs, val_loss, 'o-', color='g', label='Validation loss')\n",
        "    ax_loss.set_title('Learning Curve - Log Loss')\n",
        "    ax_loss.set_xlabel('Training Examples')\n",
        "    ax_loss.set_ylabel('Log Loss')\n",
        "    ax_loss.legend(loc='best')\n",
        "    ax_loss.grid(True)\n",
        "\n",
        "    # Adjust layout and save figures\n",
        "    fig_acc.tight_layout()\n",
        "    fig_acc.savefig('output_logistic/accuracy_learning_curve.png')\n",
        "\n",
        "    fig_loss.tight_layout()\n",
        "    fig_loss.savefig('output_logistic/loss_learning_curve.png')\n",
        "\n",
        "    plt.close('all')\n",
        "\n",
        "def create_regularization_plot(model_results, metric_name='accuracy'):\n",
        "    \"\"\"\n",
        "    Create a plot showing the effect of regularization strength\n",
        "\n",
        "    Args:\n",
        "        model_results: Dictionary with model metrics\n",
        "        metric_name: Metric to plot (accuracy or loss)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Extract data from results\n",
        "    C_values = []\n",
        "    train_metrics = []\n",
        "    val_metrics = []\n",
        "\n",
        "    for C, metrics in model_results.items():\n",
        "        C_values.append(C)\n",
        "        train_metrics.append(metrics[f'train_{metric_name}'])\n",
        "        val_metrics.append(metrics[f'val_{metric_name}'])\n",
        "\n",
        "    # Sort by C value\n",
        "    sorted_data = sorted(zip(C_values, train_metrics, val_metrics))\n",
        "    C_values, train_metrics, val_metrics = zip(*sorted_data)\n",
        "\n",
        "    # Plot\n",
        "    plt.plot(C_values, train_metrics, 'ro-', label='Training')\n",
        "    plt.plot(C_values, val_metrics, 'go-', label='Validation')\n",
        "\n",
        "    # Set axis labels and title\n",
        "    plt.xlabel('Regularization Strength (C)')\n",
        "    plt.xscale('log')\n",
        "    plt.ylabel(metric_name.capitalize())\n",
        "    plt.title(f'Effect of Regularization on {metric_name.capitalize()}')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    # Save figure\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'output_logistic/regularization_{metric_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "def visualize_feature_importance(model, feature_names):\n",
        "    \"\"\"\n",
        "    Visualize feature coefficients for logistic regression\n",
        "\n",
        "    Args:\n",
        "        model: Trained logistic regression model\n",
        "        feature_names: Names of the features\n",
        "    \"\"\"\n",
        "    # Get coefficients\n",
        "    coefs = model.coef_[0]\n",
        "\n",
        "    # Create DataFrame for visualization\n",
        "    coef_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Coefficient': coefs,\n",
        "        'Abs_Coefficient': np.abs(coefs)\n",
        "    })\n",
        "\n",
        "    # Sort by absolute coefficient value\n",
        "    coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Plot bars with positive/negative colors\n",
        "    bars = plt.barh(coef_df['Feature'], coef_df['Coefficient'])\n",
        "\n",
        "    # Color bars based on coefficient sign\n",
        "    for i, bar in enumerate(bars):\n",
        "        if coef_df['Coefficient'].iloc[i] < 0:\n",
        "            bar.set_color('r')\n",
        "        else:\n",
        "            bar.set_color('g')\n",
        "\n",
        "    plt.title('Logistic Regression Coefficients')\n",
        "    plt.xlabel('Coefficient Value')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "\n",
        "    # Save figure\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('output_logistic/feature_coefficients.png')\n",
        "    plt.close()\n",
        "\n",
        "def logistic_regression_pipeline(data_path):\n",
        "    \"\"\"\n",
        "    Run a pipeline to create a logistic regression model\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Insurance Enrollment Model with Logistic Regression ===\\n\")\n",
        "\n",
        "    # Step 1: Load the data\n",
        "    print(\"Step 1: Loading data...\")\n",
        "    data = pd.read_csv(data_path)\n",
        "    print(f\"Data loaded successfully with shape: {data.shape}\")\n",
        "\n",
        "    # Step 2: Basic data exploration\n",
        "    print(\"\\nStep 2: Basic data exploration...\")\n",
        "    print(\"Target distribution:\")\n",
        "    print(data['enrolled'].value_counts(normalize=True))\n",
        "\n",
        "    # Step 3: Feature Engineering and Preprocessing\n",
        "    print(\"\\nStep 3: Feature engineering and preprocessing...\")\n",
        "\n",
        "    # Create a copy of the dataset\n",
        "    data_processed = data.copy()\n",
        "\n",
        "    # Identify column types\n",
        "    id_cols = ['employee_id']\n",
        "    numeric_cols = ['age', 'salary', 'tenure_years']\n",
        "    categorical_cols = ['gender', 'marital_status', 'employment_type', 'region', 'has_dependents']\n",
        "    target_col = 'enrolled'\n",
        "\n",
        "    # Handle categorical variables - convert to proper format\n",
        "    data_processed['has_dependents'] = (data_processed['has_dependents'] == 'Yes').astype(int)\n",
        "\n",
        "    # Create feature transformers\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Column transformer\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ],\n",
        "        remainder='drop'  # Drop other columns\n",
        "    )\n",
        "\n",
        "    # Define X and y\n",
        "    X = data_processed.drop(columns=id_cols + [target_col])\n",
        "    y = data_processed[target_col]\n",
        "\n",
        "    # Get the feature names for the transformed data\n",
        "    categorical_features = []\n",
        "    for col in categorical_cols:\n",
        "        unique_values = data_processed[col].unique()\n",
        "        # We drop the first category in OneHotEncoder\n",
        "        categorical_features.extend([f\"{col}_{val}\" for val in unique_values[1:]])\n",
        "\n",
        "    all_features = numeric_cols + categorical_features\n",
        "\n",
        "    # Step 4: Create three separate datasets\n",
        "    print(\"\\nStep 4: Creating training, validation, and test sets...\")\n",
        "\n",
        "    # First split off test set (20%)\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42, stratify=y)\n",
        "\n",
        "    # Then split remaining data into train and validation (validation is 25% of original)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.3125,\n",
        "                                                     random_state=42, stratify=y_temp)\n",
        "\n",
        "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "    # Step 5: Preprocess the data\n",
        "    print(\"\\nStep 5: Preprocessing the data...\")\n",
        "\n",
        "    # Fit the preprocessor and transform the data\n",
        "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "    X_val_preprocessed = preprocessor.transform(X_val)\n",
        "    X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "    print(f\"Preprocessed training data shape: {X_train_preprocessed.shape}\")\n",
        "\n",
        "    # Step 6: Apply class balancing\n",
        "    print(\"\\nStep 6: Applying class balancing...\")\n",
        "\n",
        "    # Separate majority and minority classes\n",
        "    majority_indices = np.where(y_train == 1)[0]\n",
        "    minority_indices = np.where(y_train == 0)[0]\n",
        "\n",
        "    # Downsample majority class to be slightly smaller than minority\n",
        "    minority_size = len(minority_indices)\n",
        "    downsampled_size = int(minority_size * 0.9)  # 90% of minority size\n",
        "\n",
        "    # Randomly select indices from majority class\n",
        "    downsampled_indices = np.random.choice(majority_indices, size=downsampled_size, replace=False)\n",
        "\n",
        "    # Combine minority and downsampled majority indices\n",
        "    balanced_indices = np.concatenate([downsampled_indices, minority_indices])\n",
        "\n",
        "    # Create balanced datasets\n",
        "    X_train_balanced = X_train_preprocessed[balanced_indices]\n",
        "    y_train_balanced = y_train.iloc[balanced_indices].reset_index(drop=True)\n",
        "\n",
        "    print(f\"Original training class distribution: {y_train.value_counts(normalize=True)}\")\n",
        "    print(f\"Balanced training class distribution: {y_train_balanced.value_counts(normalize=True)}\")\n",
        "\n",
        "    # Step 7: Add random noise to improve robustness\n",
        "    print(\"\\nStep 7: Adding random noise to training data...\")\n",
        "\n",
        "    # Create a copy to avoid modifying original data\n",
        "    X_train_noisy = X_train_balanced.copy()\n",
        "\n",
        "    # Add noise to numerical features only\n",
        "    np.random.seed(42)\n",
        "    noise_level = 0.2\n",
        "\n",
        "    # The first few columns are the numeric features\n",
        "    num_numeric = len(numeric_cols)\n",
        "    for i in range(num_numeric):\n",
        "        noise = np.random.normal(0, noise_level, size=X_train_noisy.shape[0])\n",
        "        X_train_noisy[:, i] = X_train_noisy[:, i] + noise\n",
        "\n",
        "    # Step 8: Train logistic regression models with different regularization strengths\n",
        "    print(\"\\nStep 8: Training logistic regression models with different regularization strengths...\")\n",
        "\n",
        "    # Try different C values\n",
        "    C_values = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]\n",
        "\n",
        "    log_reg_models = {}\n",
        "    model_results = {}\n",
        "\n",
        "    for C in C_values:\n",
        "        print(f\"Regularization strength C={C}...\")\n",
        "\n",
        "        # Create a logistic regression model with L2 regularization\n",
        "        log_reg = LogisticRegression(\n",
        "            C=C,\n",
        "            penalty='l2',\n",
        "            solver='liblinear',\n",
        "            random_state=42,\n",
        "            max_iter=2000\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        log_reg.fit(X_train_noisy, y_train_balanced)\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_acc = log_reg.score(X_train_balanced, y_train_balanced)  # Use balanced but not noisy data\n",
        "        val_acc = log_reg.score(X_val_preprocessed, y_val)\n",
        "\n",
        "        train_probs = log_reg.predict_proba(X_train_balanced)\n",
        "        val_probs = log_reg.predict_proba(X_val_preprocessed)\n",
        "        train_loss = log_loss(y_train_balanced, train_probs)\n",
        "        val_loss = log_loss(y_val, val_probs)\n",
        "\n",
        "        # Store model and results\n",
        "        model_name = f\"Logistic Regression (C={C})\"\n",
        "        log_reg_models[model_name] = log_reg\n",
        "        model_results[C] = {\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_accuracy': val_acc,\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss\n",
        "        }\n",
        "\n",
        "        print(f\"Training accuracy: {train_acc:.4f}, Validation accuracy: {val_acc:.4f}\")\n",
        "        print(f\"Training loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Step 9: Create visualizations\n",
        "    print(\"\\nStep 9: Creating visualizations...\")\n",
        "\n",
        "    # Get feature names for transformed data\n",
        "    feature_names = []\n",
        "    feature_names.extend(numeric_cols)  # Add numeric column names\n",
        "\n",
        "    # Add one-hot encoded feature names\n",
        "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
        "    # Get the categories for each categorical column\n",
        "    for i, col in enumerate(categorical_cols):\n",
        "        categories = ohe.categories_[i][1:]  # Skip first category (dropped)\n",
        "        feature_names.extend([f\"{col}_{cat}\" for cat in categories])\n",
        "\n",
        "    # Create regularization plots\n",
        "    create_regularization_plot(model_results, 'accuracy')\n",
        "    create_regularization_plot(model_results, 'loss')\n",
        "\n",
        "    # Step 10: Select the best model\n",
        "    print(\"\\nStep 10: Selecting the best model...\")\n",
        "\n",
        "    # Find best model based on validation accuracy\n",
        "    best_C = max(model_results, key=lambda C: model_results[C]['val_accuracy'])\n",
        "    best_model = log_reg_models[f\"Logistic Regression (C={best_C})\"]\n",
        "    best_val_acc = model_results[best_C]['val_accuracy']\n",
        "\n",
        "    print(f\"Best model: C={best_C}, Validation accuracy: {best_val_acc:.4f}\")\n",
        "    train_val_gap = model_results[best_C]['train_accuracy'] - best_val_acc\n",
        "    print(f\"Training-validation accuracy gap: {train_val_gap:.4f}\")\n",
        "\n",
        "    # Create learning curves for the best model\n",
        "    create_learning_curves(best_model, X_train_balanced, y_train_balanced, X_val_preprocessed, y_val)\n",
        "\n",
        "    # Visualize feature importance\n",
        "    visualize_feature_importance(best_model, feature_names)\n",
        "\n",
        "    # Step 11: Evaluate on test set\n",
        "    print(\"\\nStep 11: Evaluating the best model on test set...\")\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = best_model.predict(X_test_preprocessed)\n",
        "    y_prob = best_model.predict_proba(X_test_preprocessed)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    test_accuracy = (y_pred == y_test).mean()\n",
        "    test_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Test ROC AUC: {test_auc:.4f}\")\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Step 12: Final visualizations\n",
        "    print(\"\\nStep 12: Creating final visualizations...\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix - Logistic Regression')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.savefig('output_logistic/confusion_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    # ROC curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (area = {test_auc:.4f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve - Logistic Regression')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig('output_logistic/roc_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Precision-Recall curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "    plt.plot(recall, precision, label=f'Precision-Recall curve')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve - Logistic Regression')\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.savefig('output_logistic/precision_recall_curve.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Step 13: Save the model\n",
        "    print(\"\\nStep 13: Saving the model...\")\n",
        "\n",
        "    model_info = {\n",
        "        'model': best_model,\n",
        "        'preprocessor': preprocessor,\n",
        "        'feature_names': feature_names,\n",
        "        'accuracy': test_accuracy,\n",
        "        'roc_auc': test_auc\n",
        "    }\n",
        "\n",
        "    model_path = 'output_logistic/final_model.pkl'\n",
        "    joblib.dump(model_info, model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    # Step 14: Create prediction function\n",
        "    print(\"\\nStep 14: Creating prediction function...\")\n",
        "\n",
        "    def predict_enrollment(new_data):\n",
        "        \"\"\"\n",
        "        Predict enrollment probability for new employees\n",
        "\n",
        "        Args:\n",
        "            new_data (pandas.DataFrame): Employee data\n",
        "\n",
        "        Returns:\n",
        "            pandas.DataFrame: Original data with enrollment probabilities\n",
        "        \"\"\"\n",
        "        # Create a copy of the input data\n",
        "        result = new_data.copy()\n",
        "\n",
        "        # Preprocess the data using the same preprocessor\n",
        "        processed_data = preprocessor.transform(new_data)\n",
        "\n",
        "        # Make predictions\n",
        "        enrollment_prob = best_model.predict_proba(processed_data)[:, 1]\n",
        "\n",
        "        # Add predictions to the original data\n",
        "        result['enrollment_probability'] = enrollment_prob\n",
        "        result['predicted_enrollment'] = (enrollment_prob >= 0.5).astype(int)\n",
        "\n",
        "        return result\n",
        "\n",
        "    # Step 15: Test the prediction function\n",
        "    print(\"\\nStep 15: Testing prediction function with example data...\")\n",
        "\n",
        "    # Create example data\n",
        "    example_data = pd.DataFrame({\n",
        "        'employee_id': [20001, 20002, 20003, 20004],\n",
        "        'age': [35, 50, 42, 29],\n",
        "        'gender': ['Male', 'Female', 'Other', 'Female'],\n",
        "        'marital_status': ['Married', 'Single', 'Divorced', 'Single'],\n",
        "        'salary': [75000, 85000, 62000, 45000],\n",
        "        'employment_type': ['Full-time', 'Part-time', 'Full-time', 'Contract'],\n",
        "        'region': ['West', 'Northeast', 'South', 'Midwest'],\n",
        "        'has_dependents': ['Yes', 'No', 'Yes', 'No'],\n",
        "        'tenure_years': [5.5, 2.0, 8.3, 1.2]\n",
        "    })\n",
        "\n",
        "    # Make predictions\n",
        "    example_predictions = predict_enrollment(example_data)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nExample prediction results:\")\n",
        "    print(example_predictions[['employee_id', 'employment_type', 'has_dependents',\n",
        "                             'salary', 'enrollment_probability', 'predicted_enrollment']])\n",
        "\n",
        "    # Step 16: Feature analysis\n",
        "    print(\"\\nStep 16: Feature importance analysis...\")\n",
        "\n",
        "    # Get coefficients from the model\n",
        "    coefs = best_model.coef_[0]\n",
        "\n",
        "    # Create DataFrame with feature names and coefficients\n",
        "    coef_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Coefficient': coefs,\n",
        "        'Absolute_Value': np.abs(coefs)\n",
        "    })\n",
        "\n",
        "    # Sort by absolute coefficient value\n",
        "    coef_df = coef_df.sort_values('Absolute_Value', ascending=False)\n",
        "\n",
        "    # Print top positive features\n",
        "    print(\"\\nTop positive factors for enrollment:\")\n",
        "    pos_features = coef_df[coef_df['Coefficient'] > 0].head(5)\n",
        "    for idx, row in pos_features.iterrows():\n",
        "        print(f\"  {row['Feature']}: +{row['Coefficient']:.4f}\")\n",
        "\n",
        "    # Print top negative features\n",
        "    print(\"\\nTop negative factors for enrollment:\")\n",
        "    neg_features = coef_df[coef_df['Coefficient'] < 0].head(5)\n",
        "    for idx, row in neg_features.iterrows():\n",
        "        print(f\"  {row['Feature']}: {row['Coefficient']:.4f}\")\n",
        "\n",
        "    # Step 17: Summarize the process\n",
        "    print(\"\\n=== Pipeline Summary ===\")\n",
        "    pipeline_summary = {\n",
        "        'Model': f\"Logistic Regression (C={best_C})\",\n",
        "        'Feature Count': len(feature_names),\n",
        "        'Class Balancing': 'Applied (aggressive downsampling)',\n",
        "        'Training Set Size': X_train.shape[0],\n",
        "        'Validation Set Size': X_val.shape[0],\n",
        "        'Test Set Size': X_test.shape[0],\n",
        "        'Test Accuracy': test_accuracy,\n",
        "        'Test ROC AUC': test_auc\n",
        "    }\n",
        "\n",
        "    for key, value in pipeline_summary.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    print(\"\\nLogistic regression model pipeline completed successfully!\")\n",
        "\n",
        "    return predict_enrollment\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the pipeline\n",
        "    data_path = 'employee_data.csv'  # Replace with your data path\n",
        "    predict_function = logistic_regression_pipeline(data_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jMGrVLg0p6-",
        "outputId": "099a6b91-e80c-4900-ef59-37fa14c17ff0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Insurance Enrollment Model with Logistic Regression ===\n",
            "\n",
            "Step 1: Loading data...\n",
            "Data loaded successfully with shape: (10000, 10)\n",
            "\n",
            "Step 2: Basic data exploration...\n",
            "Target distribution:\n",
            "enrolled\n",
            "1    0.6174\n",
            "0    0.3826\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Step 3: Feature engineering and preprocessing...\n",
            "\n",
            "Step 4: Creating training, validation, and test sets...\n",
            "Training set: 5500 samples\n",
            "Validation set: 2500 samples\n",
            "Test set: 2000 samples\n",
            "\n",
            "Step 5: Preprocessing the data...\n",
            "Preprocessed training data shape: (5500, 14)\n",
            "\n",
            "Step 6: Applying class balancing...\n",
            "Original training class distribution: enrolled\n",
            "1    0.617455\n",
            "0    0.382545\n",
            "Name: proportion, dtype: float64\n",
            "Balanced training class distribution: enrolled\n",
            "0    0.526395\n",
            "1    0.473605\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Step 7: Adding random noise to training data...\n",
            "\n",
            "Step 8: Training logistic regression models with different regularization strengths...\n",
            "Regularization strength C=0.0001...\n",
            "Training accuracy: 0.7896, Validation accuracy: 0.8080\n",
            "Training loss: 0.6659, Validation loss: 0.6637\n",
            "Regularization strength C=0.001...\n",
            "Training accuracy: 0.8281, Validation accuracy: 0.8356\n",
            "Training loss: 0.5466, Validation loss: 0.5384\n",
            "Regularization strength C=0.01...\n",
            "Training accuracy: 0.8804, Validation accuracy: 0.8776\n",
            "Training loss: 0.3602, Validation loss: 0.3563\n",
            "Regularization strength C=0.1...\n",
            "Training accuracy: 0.8887, Validation accuracy: 0.8808\n",
            "Training loss: 0.2534, Validation loss: 0.2619\n",
            "Regularization strength C=1.0...\n",
            "Training accuracy: 0.8914, Validation accuracy: 0.8828\n",
            "Training loss: 0.2265, Validation loss: 0.2436\n",
            "Regularization strength C=10.0...\n",
            "Training accuracy: 0.8927, Validation accuracy: 0.8828\n",
            "Training loss: 0.2242, Validation loss: 0.2437\n",
            "\n",
            "Step 9: Creating visualizations...\n",
            "\n",
            "Step 10: Selecting the best model...\n",
            "Best model: C=1.0, Validation accuracy: 0.8828\n",
            "Training-validation accuracy gap: 0.0086\n",
            "\n",
            "Step 11: Evaluating the best model on test set...\n",
            "Test Accuracy: 0.8910\n",
            "Test ROC AUC: 0.9700\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.86       765\n",
            "           1       0.94      0.88      0.91      1235\n",
            "\n",
            "    accuracy                           0.89      2000\n",
            "   macro avg       0.88      0.89      0.89      2000\n",
            "weighted avg       0.90      0.89      0.89      2000\n",
            "\n",
            "\n",
            "Step 12: Creating final visualizations...\n",
            "\n",
            "Step 13: Saving the model...\n",
            "Model saved to output_logistic/final_model.pkl\n",
            "\n",
            "Step 14: Creating prediction function...\n",
            "\n",
            "Step 15: Testing prediction function with example data...\n",
            "\n",
            "Example prediction results:\n",
            "   employee_id employment_type has_dependents  salary  enrollment_probability  \\\n",
            "0        20001       Full-time            Yes   75000                0.468277   \n",
            "1        20002       Part-time             No   85000                0.090203   \n",
            "2        20003       Full-time            Yes   62000                0.241608   \n",
            "3        20004        Contract             No   45000                0.000035   \n",
            "\n",
            "   predicted_enrollment  \n",
            "0                     0  \n",
            "1                     0  \n",
            "2                     0  \n",
            "3                     0  \n",
            "\n",
            "Step 16: Feature importance analysis...\n",
            "\n",
            "Top positive factors for enrollment:\n",
            "  employment_type_Full-time: +4.9019\n",
            "  has_dependents_1: +4.8292\n",
            "  salary: +2.1864\n",
            "  age: +1.6288\n",
            "  gender_Male: +0.1656\n",
            "\n",
            "Top negative factors for enrollment:\n",
            "  employment_type_Part-time: -0.4603\n",
            "  marital_status_Widowed: -0.3488\n",
            "  marital_status_Married: -0.3086\n",
            "  marital_status_Single: -0.3045\n",
            "  region_Northeast: -0.2172\n",
            "\n",
            "=== Pipeline Summary ===\n",
            "Model: Logistic Regression (C=1.0)\n",
            "Feature Count: 14\n",
            "Class Balancing: Applied (aggressive downsampling)\n",
            "Training Set Size: 5500\n",
            "Validation Set Size: 2500\n",
            "Test Set Size: 2000\n",
            "Test Accuracy: 0.891\n",
            "Test ROC AUC: 0.9699917969887009\n",
            "\n",
            "Logistic regression model pipeline completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N4W3B2B29-H9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}